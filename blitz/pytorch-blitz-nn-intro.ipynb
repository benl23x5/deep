{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# nn.Module \n",
    "#  - A convenient way of encapsulating parameters, \n",
    "#  - With helpers for moving them to the GPU, exporting, loading etc.\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Convolution specifications.\n",
    "        #  Creating the conv specification also creates weight vector\n",
    "        #  and initializes them with small random values, with a random seed.\n",
    "        self.conv1 = nn.Conv2d(1,6,3)\n",
    "        self.conv2 = nn.Conv2d(6,16,3)\n",
    "        \n",
    "        # fully connected layers\n",
    "        self.fc1   = nn.Linear(16 * 6 * 6, 120)\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84,10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "# Creating the network initializes the weight vectors with random values,\n",
    "# with a new random seed each time.\n",
    "net = Net()\n",
    "print(net)\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 3, 3])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 576])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Show learnable parameters.\n",
    "#  This is done by the nn framework, that searches through the\n",
    "#  network specification for _parameters fields using reflection.\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "\n",
    "# Size of convolution weights in first layer.\n",
    "print(params[0].size())  # conv kernels\n",
    "print(params[1].size())  # biases\n",
    "print(params[2].size())  # conv kernels\n",
    "print(params[3].size())  # biases\n",
    "print(params[4].size())  # \n",
    "print(params[5].size())\n",
    "print(params[6].size())\n",
    "print(params[7].size())\n",
    "print(params[8].size())\n",
    "print(params[9].size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('weight', Parameter containing:\n",
       "               tensor([[[[ 0.2526,  0.3135, -0.0784],\n",
       "                         [ 0.0204,  0.1244, -0.0102],\n",
       "                         [ 0.2787,  0.2477,  0.3020]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.2295,  0.0069, -0.0217],\n",
       "                         [-0.2912, -0.2154,  0.2183],\n",
       "                         [-0.2882,  0.2601,  0.3117]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.1047, -0.1835, -0.2343],\n",
       "                         [-0.3302, -0.1199,  0.1460],\n",
       "                         [-0.2766, -0.2508,  0.0382]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.2453,  0.0481,  0.1039],\n",
       "                         [ 0.2205, -0.2978,  0.3254],\n",
       "                         [ 0.2055, -0.1040, -0.2950]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.3266, -0.1190,  0.3330],\n",
       "                         [ 0.3076, -0.2553,  0.2697],\n",
       "                         [ 0.0849,  0.1274, -0.2441]]],\n",
       "               \n",
       "               \n",
       "                       [[[ 0.0765,  0.2401,  0.0704],\n",
       "                         [-0.1415, -0.3236,  0.0790],\n",
       "                         [-0.0113,  0.0025, -0.1017]]]], requires_grad=True)),\n",
       "              ('bias',\n",
       "               Parameter containing:\n",
       "               tensor([ 0.0118, -0.1861, -0.0287, -0.1989,  0.1908,  0.3188],\n",
       "                      requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'in_channels': 1,\n",
       " 'out_channels': 6,\n",
       " 'kernel_size': (3, 3),\n",
       " 'stride': (1, 1),\n",
       " 'padding': (0, 0),\n",
       " 'dilation': (1, 1),\n",
       " 'transposed': False,\n",
       " 'output_padding': (0, 0),\n",
       " 'groups': 1,\n",
       " 'padding_mode': 'zeros'}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the full specifiction of the convolution layer.\n",
    "#   We see that it has attached parameters have been initialized\n",
    "#   to random values. The paramters field contains both weights and bias.\n",
    "net.conv1.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0525,  0.0240,  0.1360,  0.0942,  0.0413, -0.0365, -0.0871, -0.1458,\n",
      "         -0.0712, -0.1362]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Apply the network to a random input.\n",
    "input = torch.randn(1,1,32,32)\n",
    "out   = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero the gradient buffers of all parameters then backprop a random gradient. \n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0525,  0.0240,  0.1360,  0.0942,  0.0413, -0.0365, -0.0871, -0.1458,\n",
      "         -0.0712, -0.1362]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4980,  0.4245,  1.5474,  1.3388,  0.2780, -0.1623,  1.1596, -0.1084,\n",
      "          0.0507, -0.5938]])\n"
     ]
    }
   ],
   "source": [
    "# Loss functions\n",
    "output = net(input)\n",
    "target = torch.randn(10)     # dummy target for this example\n",
    "target = target.view(1, -1)  # pack it into a mini-batch of a single element.\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5856, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7fa79835c8d0>\n",
      "<AddmmBackward object at 0x7fa7aa0d12d0>\n",
      "<AccumulateGrad object at 0x7fa79835c8d0>\n"
     ]
    }
   ],
   "source": [
    "# Can print out the backward graph.\n",
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[[[ 0.2526,  0.3135, -0.0784],\n",
       "                        [ 0.0204,  0.1244, -0.0102],\n",
       "                        [ 0.2787,  0.2477,  0.3020]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2295,  0.0069, -0.0217],\n",
       "                        [-0.2912, -0.2154,  0.2183],\n",
       "                        [-0.2882,  0.2601,  0.3117]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1047, -0.1835, -0.2343],\n",
       "                        [-0.3302, -0.1199,  0.1460],\n",
       "                        [-0.2766, -0.2508,  0.0382]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2453,  0.0481,  0.1039],\n",
       "                        [ 0.2205, -0.2978,  0.3254],\n",
       "                        [ 0.2055, -0.1040, -0.2950]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3266, -0.1190,  0.3330],\n",
       "                        [ 0.3076, -0.2553,  0.2697],\n",
       "                        [ 0.0849,  0.1274, -0.2441]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0765,  0.2401,  0.0704],\n",
       "                        [-0.1415, -0.3236,  0.0790],\n",
       "                        [-0.0113,  0.0025, -0.1017]]]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([ 0.0118, -0.1861, -0.0287, -0.1989,  0.1908,  0.3188],\n",
       "                     requires_grad=True))])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0072, -0.0175, -0.0079, -0.0122, -0.0022,  0.0024])\n"
     ]
    }
   ],
   "source": [
    "# Show bias parameters being updated via backpropagation.\n",
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update all the weights using SCD.\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[[[ 0.2526,  0.3136, -0.0784],\n",
       "                        [ 0.0204,  0.1244, -0.0100],\n",
       "                        [ 0.2787,  0.2478,  0.3021]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2295,  0.0070, -0.0218],\n",
       "                        [-0.2912, -0.2155,  0.2183],\n",
       "                        [-0.2882,  0.2603,  0.3118]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1049, -0.1836, -0.2342],\n",
       "                        [-0.3304, -0.1198,  0.1461],\n",
       "                        [-0.2766, -0.2510,  0.0381]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2453,  0.0480,  0.1039],\n",
       "                        [ 0.2205, -0.2980,  0.3255],\n",
       "                        [ 0.2056, -0.1039, -0.2950]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3267, -0.1191,  0.3330],\n",
       "                        [ 0.3076, -0.2554,  0.2696],\n",
       "                        [ 0.0849,  0.1273, -0.2441]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0765,  0.2401,  0.0705],\n",
       "                        [-0.1415, -0.3235,  0.0788],\n",
       "                        [-0.0114,  0.0025, -0.1016]]]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([ 0.0119, -0.1859, -0.0286, -0.1988,  0.1908,  0.3187],\n",
       "                     requires_grad=True))])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1._parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pytorch optimisers\n",
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Destructively zero the gradient buffers.\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Run the forward pass.\n",
    "output = net(input)\n",
    "\n",
    "# Compute the loss in the forward psas.\n",
    "loss   = criterion(output, target)\n",
    "\n",
    "# Backprop loss.\n",
    "loss.backward()\n",
    "\n",
    "# Update the parameters based on the gradients currently in their gradient buffers.\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              Parameter containing:\n",
       "              tensor([[[[ 0.2527,  0.3155, -0.0794],\n",
       "                        [ 0.0202,  0.1244, -0.0092],\n",
       "                        [ 0.2801,  0.2492,  0.3038]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2302,  0.0074, -0.0228],\n",
       "                        [-0.2923, -0.2166,  0.2187],\n",
       "                        [-0.2885,  0.2620,  0.3129]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.1069, -0.1845, -0.2342],\n",
       "                        [-0.3323, -0.1198,  0.1472],\n",
       "                        [-0.2772, -0.2526,  0.0377]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.2462,  0.0478,  0.1040],\n",
       "                        [ 0.2213, -0.2989,  0.3265],\n",
       "                        [ 0.2062, -0.1030, -0.2953]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.3289, -0.1199,  0.3338],\n",
       "                        [ 0.3090, -0.2569,  0.2702],\n",
       "                        [ 0.0841,  0.1271, -0.2457]]],\n",
       "              \n",
       "              \n",
       "                      [[[ 0.0772,  0.2418,  0.0709],\n",
       "                        [-0.1422, -0.3253,  0.0786],\n",
       "                        [-0.0118,  0.0029, -0.1017]]]], requires_grad=True)),\n",
       "             ('bias',\n",
       "              Parameter containing:\n",
       "              tensor([ 0.0141, -0.1841, -0.0270, -0.1978,  0.1931,  0.3207],\n",
       "                     requires_grad=True))])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1._parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
